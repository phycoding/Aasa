{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mk-minchul/AdaFace.git"
      ],
      "metadata": {
        "id": "O4KbIgCTLx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import Sequential\n",
        "from torch.nn import Conv2d, Linear\n",
        "from torch.nn import BatchNorm1d, BatchNorm2d\n",
        "from torch.nn import ReLU, Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.nn import PReLU\n",
        "import os\n",
        "\n",
        "def build_model(model_name='ir_50'):\n",
        "    if model_name == 'ir_101':\n",
        "        return IR_101(input_size=(112,112))\n",
        "    elif model_name == 'ir_50':\n",
        "        return IR_50(input_size=(112,112))\n",
        "    elif model_name == 'ir_se_50':\n",
        "        return IR_SE_50(input_size=(112,112))\n",
        "    elif model_name == 'ir_34':\n",
        "        return IR_34(input_size=(112,112))\n",
        "    elif model_name == 'ir_18':\n",
        "        return IR_18(input_size=(112,112))\n",
        "    else:\n",
        "        raise ValueError('not a correct model name', model_name)\n",
        "\n",
        "def initialize_weights(modules):\n",
        "    \"\"\" Weight initilize, conv2d and linear is initialized with kaiming_normal\n",
        "    \"\"\"\n",
        "    for m in modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight,\n",
        "                                    mode='fan_out',\n",
        "                                    nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight,\n",
        "                                    mode='fan_out',\n",
        "                                    nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "class Flatten(Module):\n",
        "    \"\"\" Flat tensor\n",
        "    \"\"\"\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class LinearBlock(Module):\n",
        "    \"\"\" Convolution block without no-linear activation layer\n",
        "    \"\"\"\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(LinearBlock, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_c, kernel, stride, padding, groups=groups, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GNAP(Module):\n",
        "    \"\"\" Global Norm-Aware Pooling block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_c):\n",
        "        super(GNAP, self).__init__()\n",
        "        self.bn1 = BatchNorm2d(in_c, affine=False)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.bn2 = BatchNorm1d(in_c, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(x)\n",
        "        x_norm = torch.norm(x, 2, 1, True)\n",
        "        x_norm_mean = torch.mean(x_norm)\n",
        "        weight = x_norm_mean / x_norm\n",
        "        x = x * weight\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        feature = self.bn2(x)\n",
        "        return feature\n",
        "\n",
        "\n",
        "class GDC(Module):\n",
        "    \"\"\" Global Depthwise Convolution block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_c, embedding_size):\n",
        "        super(GDC, self).__init__()\n",
        "        self.conv_6_dw = LinearBlock(in_c, in_c,\n",
        "                                     groups=in_c,\n",
        "                                     kernel=(7, 7),\n",
        "                                     stride=(1, 1),\n",
        "                                     padding=(0, 0))\n",
        "        self.conv_6_flatten = Flatten()\n",
        "        self.linear = Linear(in_c, embedding_size, bias=False)\n",
        "        self.bn = BatchNorm1d(embedding_size, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_6_dw(x)\n",
        "        x = self.conv_6_flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SEModule(Module):\n",
        "    \"\"\" SE block\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, reduction):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = Conv2d(channels, channels // reduction,\n",
        "                          kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
        "\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc2 = Conv2d(channels // reduction, channels,\n",
        "                          kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlockIR(Module):\n",
        "    \"\"\" BasicBlock for IRNet\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(BasicBlockIR, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "                BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
        "            BatchNorm2d(depth),\n",
        "            PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
        "            BatchNorm2d(depth))\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class BottleneckIR(Module):\n",
        "    \"\"\" BasicBlock with bottleneck for IRNet\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(BottleneckIR, self).__init__()\n",
        "        reduction_channel = depth // 4\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "                BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, reduction_channel, (1, 1), (1, 1), 0, bias=False),\n",
        "            BatchNorm2d(reduction_channel),\n",
        "            PReLU(reduction_channel),\n",
        "            Conv2d(reduction_channel, reduction_channel, (3, 3), (1, 1), 1, bias=False),\n",
        "            BatchNorm2d(reduction_channel),\n",
        "            PReLU(reduction_channel),\n",
        "            Conv2d(reduction_channel, depth, (1, 1), stride, 0, bias=False),\n",
        "            BatchNorm2d(depth))\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class BasicBlockIRSE(BasicBlockIR):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(BasicBlockIRSE, self).__init__(in_channel, depth, stride)\n",
        "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
        "\n",
        "\n",
        "class BottleneckIRSE(BottleneckIR):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(BottleneckIRSE, self).__init__(in_channel, depth, stride)\n",
        "        self.res_layer.add_module(\"se_block\", SEModule(depth, 16))\n",
        "\n",
        "\n",
        "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
        "    '''A named tuple describing a ResNet block.'''\n",
        "\n",
        "\n",
        "def get_block(in_channel, depth, num_units, stride=2):\n",
        "\n",
        "    return [Bottleneck(in_channel, depth, stride)] +\\\n",
        "           [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
        "\n",
        "\n",
        "def get_blocks(num_layers):\n",
        "    if num_layers == 18:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=2),\n",
        "            get_block(in_channel=64, depth=128, num_units=2),\n",
        "            get_block(in_channel=128, depth=256, num_units=2),\n",
        "            get_block(in_channel=256, depth=512, num_units=2)\n",
        "        ]\n",
        "    elif num_layers == 34:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=4),\n",
        "            get_block(in_channel=128, depth=256, num_units=6),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 50:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=4),\n",
        "            get_block(in_channel=128, depth=256, num_units=14),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 100:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=13),\n",
        "            get_block(in_channel=128, depth=256, num_units=30),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 152:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=256, num_units=3),\n",
        "            get_block(in_channel=256, depth=512, num_units=8),\n",
        "            get_block(in_channel=512, depth=1024, num_units=36),\n",
        "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 200:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=256, num_units=3),\n",
        "            get_block(in_channel=256, depth=512, num_units=24),\n",
        "            get_block(in_channel=512, depth=1024, num_units=36),\n",
        "            get_block(in_channel=1024, depth=2048, num_units=3)\n",
        "        ]\n",
        "\n",
        "    return blocks\n",
        "\n",
        "\n",
        "class Backbone(Module):\n",
        "    def __init__(self, input_size, num_layers, mode='ir'):\n",
        "        \"\"\" Args:\n",
        "            input_size: input_size of backbone\n",
        "            num_layers: num_layers of backbone\n",
        "            mode: support ir or irse\n",
        "        \"\"\"\n",
        "        super(Backbone, self).__init__()\n",
        "        assert input_size[0] in [112, 224], \\\n",
        "            \"input_size should be [112, 112] or [224, 224]\"\n",
        "        assert num_layers in [18, 34, 50, 100, 152, 200], \\\n",
        "            \"num_layers should be 18, 34, 50, 100 or 152\"\n",
        "        assert mode in ['ir', 'ir_se'], \\\n",
        "            \"mode should be ir or ir_se\"\n",
        "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
        "                                      BatchNorm2d(64), PReLU(64))\n",
        "        blocks = get_blocks(num_layers)\n",
        "        if num_layers <= 100:\n",
        "            if mode == 'ir':\n",
        "                unit_module = BasicBlockIR\n",
        "            elif mode == 'ir_se':\n",
        "                unit_module = BasicBlockIRSE\n",
        "            output_channel = 512\n",
        "        else:\n",
        "            if mode == 'ir':\n",
        "                unit_module = BottleneckIR\n",
        "            elif mode == 'ir_se':\n",
        "                unit_module = BottleneckIRSE\n",
        "            output_channel = 2048\n",
        "\n",
        "        if input_size[0] == 112:\n",
        "            self.output_layer = Sequential(BatchNorm2d(output_channel),\n",
        "                                        Dropout(0.4), Flatten(),\n",
        "                                        Linear(output_channel * 7 * 7, 512),\n",
        "                                        BatchNorm1d(512, affine=False))\n",
        "        else:\n",
        "            self.output_layer = Sequential(\n",
        "                BatchNorm2d(output_channel), Dropout(0.4), Flatten(),\n",
        "                Linear(output_channel * 14 * 14, 512),\n",
        "                BatchNorm1d(512, affine=False))\n",
        "\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(\n",
        "                    unit_module(bottleneck.in_channel, bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "        self.body = Sequential(*modules)\n",
        "\n",
        "        initialize_weights(self.modules())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # current code only supports one extra image\n",
        "        # it comes with a extra dimension for number of extra image. We will just squeeze it out for now\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        for idx, module in enumerate(self.body):\n",
        "            x = module(x)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        norm = torch.norm(x, 2, 1, True)\n",
        "        output = torch.div(x, norm)\n",
        "\n",
        "        return output, norm\n",
        "\n",
        "\n",
        "\n",
        "def IR_18(input_size):\n",
        "    \"\"\" Constructs a ir-18 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 18, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_34(input_size):\n",
        "    \"\"\" Constructs a ir-34 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 34, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_50(input_size):\n",
        "    \"\"\" Constructs a ir-50 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 50, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_101(input_size):\n",
        "    \"\"\" Constructs a ir-101 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 100, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_152(input_size):\n",
        "    \"\"\" Constructs a ir-152 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 152, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_200(input_size):\n",
        "    \"\"\" Constructs a ir-200 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 200, 'ir')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_SE_50(input_size):\n",
        "    \"\"\" Constructs a ir_se-50 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 50, 'ir_se')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_SE_101(input_size):\n",
        "    \"\"\" Constructs a ir_se-101 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 100, 'ir_se')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_SE_152(input_size):\n",
        "    \"\"\" Constructs a ir_se-152 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 152, 'ir_se')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def IR_SE_200(input_size):\n",
        "    \"\"\" Constructs a ir_se-200 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 200, 'ir_se')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dE4jVwNAN2Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/AdaFace/requirements.txt"
      ],
      "metadata": {
        "id": "gQQyurp6L6aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from AdaFace.face_alignment import mtcnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQQLAR9WO4BK",
        "outputId": "f3792481-266b-4206-c550-d7c797127d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/AdaFace/face_alignment/mtcnn_pytorch/src/align_trans.py:287: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if align_type is 'cv2_affine':\n",
            "/content/AdaFace/face_alignment/mtcnn_pytorch/src/align_trans.py:290: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif align_type is 'affine':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from datetime import datetime\n",
        "mtcnn_model = mtcnn.MTCNN(device='cuda:0', crop_size=(112, 112))\n",
        "\n",
        "def add_padding(pil_img, top, right, bottom, left, color=(0,0,0)):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def get_aligned_face(image_path, rgb_pil_image=None):\n",
        "    if rgb_pil_image is None:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "    else:\n",
        "        assert isinstance(rgb_pil_image, Image.Image), 'Face alignment module requires PIL image or path to the image'\n",
        "        img = rgb_pil_image\n",
        "    # find face\n",
        "    try:\n",
        "        bboxes, faces = mtcnn_model.align_multi(img, limit=1)\n",
        "        face = faces[0]\n",
        "    except Exception as e:\n",
        "        print('Face detection Failed due to error.')\n",
        "        print(e)\n",
        "        face = None\n",
        "\n",
        "    return face"
      ],
      "metadata": {
        "id": "Be1VrOnvOijF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "1FNi65Wyj36M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from AdaFace import face_alignment"
      ],
      "metadata": {
        "id": "szU8m-J3U1NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from AdaFace.face_alignment import align"
      ],
      "metadata": {
        "id": "M4loJL_fUf9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "adaface_models = {\n",
        "    'ir_50':\"/content/drive/MyDrive/Copy of adaface_ir50_casia.ckpt\",\n",
        "}\n",
        "\n",
        "def load_pretrained_model(architecture='ir_50'):\n",
        "    # load model and pretrained statedict\n",
        "    assert architecture in adaface_models.keys()\n",
        "    model = build_model(architecture)\n",
        "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
        "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
        "    model.load_state_dict(model_statedict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def to_input(pil_rgb_image):\n",
        "    np_img = np.array(pil_rgb_image)\n",
        "    brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5\n",
        "    tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()\n",
        "    return tensor\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    model = load_pretrained_model('ir_50')\n",
        "    feature, norm = model(torch.randn(2,3,112,112))\n",
        "\n",
        "    test_image_path = '/content/AdaFace/face_alignment/test_images'\n",
        "    features = []\n",
        "    for fname in sorted(os.listdir(test_image_path)):\n",
        "        path = os.path.join(test_image_path, fname)\n",
        "        aligned_rgb_img = align.get_aligned_face(path)\n",
        "        bgr_tensor_input = to_input(aligned_rgb_img)\n",
        "        feature, _ = model(bgr_tensor_input)\n",
        "        features.append(feature)\n",
        "\n",
        "    similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
        "    print(similarity_scores)\n"
      ],
      "metadata": {
        "id": "5XVTMSlwLe0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f8e7f9-b91b-4ed7-c97f-855462bed289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/AdaFace/face_alignment/mtcnn_pytorch/src/matlab_cp2tform.py:90: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  r, _, _, _ = lstsq(X, U)\n",
            "<ipython-input-18-9c7528a6b051>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.7028, 0.0015],\n",
            "        [0.7028, 1.0000, 0.0836],\n",
            "        [0.0015, 0.0836, 1.0000]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bkw7ZKmhRvar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqlI9Cn8ShEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}